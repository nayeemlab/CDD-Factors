# -*- coding: utf-8 -*-
"""CLNML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JaQUO-K9rA3e3gVlJc9xdwonRZsuCLZ8

### **Paper title** : Predictive modeling of Children's health,Learning and Well-being in Bangladesh using Supervised Machine Learning Techniques
"""

# Importing the necessary libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score

categorical_2019 = pd.read_csv("/content/2019Cat.csv")
numerical_2019 = pd.read_csv("/content/2019Num.csv")

mics6 = categorical_2019[['CAGE','area1','melevel','windex5','Religion1','HHSEX','ethnicity','HL4','WA2','ED1','underweight1','Stunned1',
             'Wasted1','Overweight1','TF1','HH7','ECEP1','MS1','FS1','OS1','IS1','SI2','Books1','Toys1','MM1','CPU1','Literacy_numeracy_score1','Physical_score1','Learning_score1','Social_emotional_score1']]

mics6.shape

mics6.head()

# Converting child's age from month to year

def cage_years(cage):
    if cage<48:
        return 3
    elif 48<=cage<60:
        return 4

mics6['CAGE(in years)'] = mics6['CAGE'].apply(cage_years)
mics6 = mics6.drop('CAGE',axis=1)
mics6.head()

mics6.isnull().sum()

# Dropping all the null values

print(f"Shape of dataset before removing null values {mics6.shape}")
mics6 = mics6.dropna()
print(f"Shape of dataset after removing null values {mics6.shape}")

"""**We will drop some of the features based on two criteria:**

1) On the basis of correlation among each independent variable.

2) By using Variance Inflation Factor (VIF)
"""

mics6['HL4'].value_counts()

mics6['area1'].value_counts()

mics6_gender = mics6.copy()

male = mics6_gender[mics6_gender['HL4']=='MALE']
female = mics6_gender[mics6_gender['HL4']=='FEMALE']

"""**Male**"""

# Literacy numeracy

print(male['Literacy_numeracy_score1'].value_counts())

# Physical score
print(male['Physical_score1'].value_counts())

# Learning score
print(male['Learning_score1'].value_counts())


# Social emotional score
print(male['Social_emotional_score1'].value_counts())

# Literacy numeracy

print(female['Literacy_numeracy_score1'].value_counts())

# Physical score
print(female['Physical_score1'].value_counts())

# Learning score
print(female['Learning_score1'].value_counts())


# Social emotional score
print(female['Social_emotional_score1'].value_counts())







mics_encoded = mics6.copy()

# Label encoding the categorical features

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
mics_encoded['area1'] = le.fit_transform(mics_encoded['area1'])
mics_encoded['melevel'] = le.fit_transform(mics_encoded['melevel'])
mics_encoded['windex5'] = le.fit_transform(mics_encoded['windex5'])
mics_encoded['Religion1'] = le.fit_transform(mics_encoded['Religion1'])
mics_encoded['HHSEX'] = le.fit_transform(mics_encoded['HHSEX'])
mics_encoded['ethnicity'] = le.fit_transform(mics_encoded['ethnicity'])
mics_encoded['HL4'] = le.fit_transform(mics_encoded['HL4'])
mics_encoded['underweight1'] = le.fit_transform(mics_encoded['underweight1'])
mics_encoded['Stunned1'] = le.fit_transform(mics_encoded['Stunned1'])
mics_encoded['Wasted1'] = le.fit_transform(mics_encoded['Wasted1'])
mics_encoded['Overweight1'] = le.fit_transform(mics_encoded['Overweight1'])
mics_encoded['TF1'] = le.fit_transform(mics_encoded['TF1'])
mics_encoded['ECEP1'] = le.fit_transform(mics_encoded['ECEP1'])
mics_encoded['IS1'] = le.fit_transform(mics_encoded['IS1'])
mics_encoded['Books1'] = le.fit_transform(mics_encoded['Books1'])
mics_encoded['Toys1'] = le.fit_transform(mics_encoded['Toys1'])
mics_encoded['MM1'] = le.fit_transform(mics_encoded['MM1'])
mics_encoded['HH7'] = le.fit_transform(mics_encoded['HH7'])
mics_encoded['Literacy_numeracy_score1'] = le.fit_transform(mics_encoded['Literacy_numeracy_score1'])
mics_encoded['Physical_score1'] = le.fit_transform(mics_encoded['Physical_score1'])
mics_encoded['Learning_score1'] = le.fit_transform(mics_encoded['Learning_score1'])
mics_encoded['Social_emotional_score1'] = le.fit_transform(mics_encoded['Social_emotional_score1'])

mics_encoded.head()

plt.figure(figsize=(20,10))
sns.heatmap(mics_encoded.corr(),annot=True)
plt.show()

# Calculating VIF

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(dataset):
    vif = pd.DataFrame()
    vif['feature'] = dataset.columns
    vif['VIF_values'] = [variance_inflation_factor(dataset.values,i) for i in range(dataset.shape[1])]
    return vif

features = mics_encoded.drop(['Literacy_numeracy_score1', 'Social_emotional_score1', 'Physical_score1', 'Learning_score1'],axis=1)
calculate_vif(features)

"""**So it is clearly seen that CPU1 and CAGE(in years) has a very high VIF values. So we are going to drop one of these. Which one should be dropped? For this, we are going to see which variable is more correlated to dependent variable(s). The one with low correlation value is going to be dropped!**"""

mics_encoded.corr()

"""So we have found that CPU1 is less correlated to dependent variable(s) and that's why we are going to drop CPU1 and also since WA2 has VIF>9 so we will also drop it."""

mics_encoded_revised = mics_encoded.drop(['CPU1','WA2','FS1','MS1','OS1','SI2','ED1'],axis=1)

mics_encoded_revised.shape

# Let's see the VIFs again!

features = mics_encoded_revised.drop(['Literacy_numeracy_score1', 'Social_emotional_score1', 'Physical_score1', 'Learning_score1'],axis=1)
calculate_vif(features)

"""# Cross tabs

### Litercy-Numeracy
"""

pd.crosstab(mics_encoded['CAGE(in years)'],mics_encoded['Literacy_numeracy_score1'])

pd.crosstab(mics6['area1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['windex5'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Religion1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['HHSEX'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['ethnicity'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['HL4'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['underweight1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Stunned1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Wasted1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Overweight1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['TF1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['HH7'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['ECEP1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['IS1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Books1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['Toys1'],mics6['Literacy_numeracy_score1'])

pd.crosstab(mics6['MM1'],mics6['Literacy_numeracy_score1'])

"""### Physical"""

pd.crosstab(mics_encoded['CAGE(in years)'],mics_encoded['Physical_score1'])

pd.crosstab(mics6['area1'],mics6['Physical_score1'])

pd.crosstab(mics6['melevel'],mics6['Physical_score1'])

pd.crosstab(mics6['windex5'],mics6['Physical_score1'])

pd.crosstab(mics6['Religion1'],mics6['Physical_score1'])

pd.crosstab(mics6['HHSEX'],mics6['Physical_score1'])

pd.crosstab(mics6['ethnicity'],mics6['Physical_score1'])

pd.crosstab(mics6['HL4'],mics6['Physical_score1'])

pd.crosstab(mics6['underweight1'],mics6['Physical_score1'])

pd.crosstab(mics6['Stunned1'],mics6['Physical_score1'])

pd.crosstab(mics6['Wasted1'],mics6['Physical_score1'])

pd.crosstab(mics6['Overweight1'],mics6['Physical_score1'])

pd.crosstab(mics6['TF1'],mics6['Physical_score1'])

pd.crosstab(mics6['HH7'],mics6['Physical_score1'])

pd.crosstab(mics6['ECEP1'],mics6['Physical_score1'])

pd.crosstab(mics6['IS1'],mics6['Physical_score1'])

pd.crosstab(mics6['Books1'],mics6['Physical_score1'])

pd.crosstab(mics6['Toys1'],mics6['Physical_score1'])

pd.crosstab(mics6['MM1'],mics6['Physical_score1'])

"""### Learning"""

pd.crosstab(mics_encoded['CAGE(in years)'],mics_encoded['Learning_score1'])

pd.crosstab(mics6['area1'],mics6['Learning_score1'])

pd.crosstab(mics6['windex5'],mics6['Learning_score1'])

pd.crosstab(mics6['Religion1'],mics6['Learning_score1'])

pd.crosstab(mics6['HHSEX'],mics6['Learning_score1'])

pd.crosstab(mics6['ethnicity'],mics6['Learning_score1'])

pd.crosstab(mics6['HL4'],mics6['Learning_score1'])

pd.crosstab(mics6['underweight1'],mics6['Learning_score1'])

pd.crosstab(mics6['Stunned1'],mics6['Learning_score1'])

pd.crosstab(mics6['Wasted1'],mics6['Learning_score1'])

pd.crosstab(mics6['Wasted1'],mics6['Learning_score1'])

pd.crosstab(mics6['Overweight1'],mics6['Learning_score1'])

pd.crosstab(mics6['TF1'],mics6['Learning_score1'])

pd.crosstab(mics6['HH7'],mics6['Learning_score1'])

pd.crosstab(mics6['ECEP1'],mics6['Learning_score1'])

pd.crosstab(mics6['IS1'],mics6['Learning_score1'])

pd.crosstab(mics6['Books1'],mics6['Learning_score1'])

pd.crosstab(mics6['Toys1'],mics6['Learning_score1'])

pd.crosstab(mics6['MM1'],mics6['Learning_score1'])

"""### Social-Emotional"""

pd.crosstab(mics_encoded['CAGE(in years)'],mics_encoded['Social_emotional_score1'])

pd.crosstab(mics6['area1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['melevel'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['windex5'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Religion1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['HHSEX'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['ethnicity'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['HL4'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['underweight1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Stunned1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Wasted1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Overweight1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['TF1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['HH7'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['ECEP1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['IS1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Books1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['Toys1'],mics6['Social_emotional_score1'])

pd.crosstab(mics6['MM1'],mics6['Social_emotional_score1'])

mics_final = mics_encoded_revised.copy()

mics_final['Literacy_numeracy_score1'].value_counts()


# Literacy Numeracy percentage - 28.63%

mics_final['Physical_score1'].value_counts()

# Physical score percentage - 98.70%

mics_final['Learning_score1'].value_counts()

# Learning score percentage - 90.6%

mics_final['Social_emotional_score1'].value_counts()

# Social emotional score percentage - 72.43%

"""# Modeling

## Literacy-Numeracy
"""

lit_num = mics_final.drop(['Physical_score1','Learning_score1','Social_emotional_score1'],axis=1)

x = lit_num.drop('Literacy_numeracy_score1',axis=1)
y = lit_num['Literacy_numeracy_score1']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

"""**We are using SMOTE here.SMOTE (Synthetic Minority Over-sampling Technique) is a method used in machine learning to address class imbalance in datasets. It works by generating synthetic examples for the minority class, effectively increasing its representation. SMOTE does this by selecting random points from the minority class and creating new, synthetic samples along the line segments joining these points and their nearest neighbors. This approach helps to create a more balanced dataset, which can improve the performance of classifiers by preventing them from being biased towards the majority class. SMOTE is particularly useful in situations where the number of instances in the minority class is too small to train an effective model.**"""

# CART

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline as ImbPipeline



pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', DecisionTreeClassifier())
])

param_grid = {
    'classifier__criterion': ['gini', 'entropy'],
    'classifier__max_depth': [None, 10, 20, 30, 40, 50],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': [None, 'sqrt', 'log2'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

try:
    feature_importances = best_estimator.named_steps['classifier'].feature_importances_
    features = x_train.columns
    importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
    importance_df = importance_df.sort_values('importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
    plt.title('Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
except AttributeError:
    print("Feature importances are not available for this model.")

# Random Forest

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': ['auto', 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# XGBoost

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
])

param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__max_depth': [3, 6, 9],
    'classifier__min_child_weight': [1, 3, 5],
    'classifier__gamma': [0, 0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Logistic Regression

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import pandas as pd


pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(solver='saga', random_state=42, max_iter=5000))
])

param_grid = {
    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__l1_ratio': [0, 0.25, 0.5, 0.75, 1]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].coef_[0]
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (Logistic Regression Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# SVM

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

svm = SVC(probability=True, kernel='linear')

svm.fit(x_train_resampled, y_train_resampled)

y_train_pred = svm.predict(x_train_resampled)
y_test_pred = svm.predict(x_test)

train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train_resampled, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train_resampled, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train_resampled, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train_resampled, svm.predict_proba(x_train_resampled)[:, 1])
test_roc_auc = roc_auc_score(y_test, svm.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = np.abs(svm.coef_[0])
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (SVM Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## Physical"""

phy = mics_final.drop(['Literacy_numeracy_score1','Learning_score1','Social_emotional_score1'],axis=1)

x = phy.drop('Physical_score1',axis=1)
y = phy['Physical_score1']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

# CART

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline as ImbPipeline



pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', DecisionTreeClassifier())
])

param_grid = {
    'classifier__criterion': ['gini', 'entropy'],
    'classifier__max_depth': [None, 10, 20, 30, 40, 50],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': [None, 'sqrt', 'log2'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

try:
    feature_importances = best_estimator.named_steps['classifier'].feature_importances_
    features = x_train.columns
    importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
    importance_df = importance_df.sort_values('importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
    plt.title('Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
except AttributeError:
    print("Feature importances are not available for this model.")

# Random Forest

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': ['auto', 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# XGBoost

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
])

param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__max_depth': [3, 6, 9],
    'classifier__min_child_weight': [1, 3, 5],
    'classifier__gamma': [0, 0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Logistic Regression

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score


pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(solver='saga', random_state=42, max_iter=5000))
])

param_grid = {
    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__l1_ratio': [0, 0.25, 0.5, 0.75, 1]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].coef_[0]
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (Logistic Regression Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# SVM

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

svm = SVC(probability=True, kernel='linear')

svm.fit(x_train_resampled, y_train_resampled)

y_train_pred = svm.predict(x_train_resampled)
y_test_pred = svm.predict(x_test)

train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train_resampled, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train_resampled, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train_resampled, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train_resampled, svm.predict_proba(x_train_resampled)[:, 1])
test_roc_auc = roc_auc_score(y_test, svm.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = np.abs(svm.coef_[0])
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (SVM Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## Learning"""

learning = mics_final.drop(['Literacy_numeracy_score1','Physical_score1','Social_emotional_score1'],axis=1)

x = learning.drop('Learning_score1',axis=1)
y = learning['Learning_score1']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

# CART

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline as ImbPipeline



pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', DecisionTreeClassifier())
])

param_grid = {
    'classifier__criterion': ['gini', 'entropy'],
    'classifier__max_depth': [None, 10, 20, 30, 40, 50],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': [None, 'sqrt', 'log2'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

try:
    feature_importances = best_estimator.named_steps['classifier'].feature_importances_
    features = x_train.columns
    importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
    importance_df = importance_df.sort_values('importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
    plt.title('Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
except AttributeError:
    print("Feature importances are not available for this model.")

# Random Forest

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': ['auto', 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# XGBoost

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
])

param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__max_depth': [3, 6, 9],
    'classifier__min_child_weight': [1, 3, 5],
    'classifier__gamma': [0, 0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Logistic Regression

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import pandas as pd


pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(solver='saga', random_state=42, max_iter=5000))
])

param_grid = {
    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__l1_ratio': [0, 0.25, 0.5, 0.75, 1]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].coef_[0]
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (Logistic Regression Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# SVM

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

svm = SVC(probability=True, kernel='linear')

svm.fit(x_train_resampled, y_train_resampled)

y_train_pred = svm.predict(x_train_resampled)
y_test_pred = svm.predict(x_test)

train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train_resampled, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train_resampled, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train_resampled, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train_resampled, svm.predict_proba(x_train_resampled)[:, 1])
test_roc_auc = roc_auc_score(y_test, svm.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = np.abs(svm.coef_[0])
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (SVM Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## Social-Emotional"""

social_emotional = mics_final.drop(['Literacy_numeracy_score1','Physical_score1','Learning_score1'],axis=1)

x = social_emotional.drop('Social_emotional_score1',axis=1)
y = social_emotional['Social_emotional_score1']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

# CART

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline as ImbPipeline



pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', DecisionTreeClassifier())
])

param_grid = {
    'classifier__criterion': ['gini', 'entropy'],
    'classifier__max_depth': [None, 10, 20, 30, 40, 50],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__max_features': [None, 'sqrt', 'log2'],
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

try:
    feature_importances = best_estimator.named_steps['classifier'].feature_importances_
    features = x_train.columns
    importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
    importance_df = importance_df.sort_values('importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
    plt.title('Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()
except AttributeError:
    print("Feature importances are not available for this model.")

from sklearn.pipeline import Pipeline
from imblearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from imblearn.over_sampling import SMOTE


pipeline = make_pipeline(
    SMOTE(random_state=42),
    RandomForestClassifier(random_state=42)
)

param_grid = {
    'randomforestclassifier__n_estimators': [100, 200, 300],
    'randomforestclassifier__max_depth': [None, 10, 20, 30],
    'randomforestclassifier__min_samples_split': [2, 5, 10],
    'randomforestclassifier__min_samples_leaf': [1, 2, 4],
    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2']
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['randomforestclassifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# XGBoost

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score


pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
])

param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__learning_rate': [0.01, 0.1, 0.2],
    'classifier__max_depth': [3, 6, 9],
    'classifier__min_child_weight': [1, 3, 5],
    'classifier__gamma': [0, 0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)


train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].feature_importances_
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Logistic Regression

from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import pandas as pd


pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(solver='saga', random_state=42, max_iter=5000))
])

param_grid = {
    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__l1_ratio': [0, 0.25, 0.5, 0.75, 1]
}

grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best parameters found: ", best_params)

cv_scores = cross_val_score(best_estimator, x_train, y_train, cv=5)
mean_cv_score = cv_scores.mean()
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", mean_cv_score)

best_estimator.fit(x_train, y_train)
y_train_pred = best_estimator.predict(x_train)
y_test_pred = best_estimator.predict(x_test)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train, best_estimator.predict_proba(x_train)[:, 1])
test_roc_auc = roc_auc_score(y_test, best_estimator.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = best_estimator.named_steps['classifier'].coef_[0]
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (Logistic Regression Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# SVM

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE


smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

svm = SVC(probability=True, kernel='linear')

svm.fit(x_train_resampled, y_train_resampled)

y_train_pred = svm.predict(x_train_resampled)
y_test_pred = svm.predict(x_test)

train_accuracy = accuracy_score(y_train_resampled, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Training accuracy:", train_accuracy)
print("Testing accuracy:", test_accuracy)

train_conf_matrix = confusion_matrix(y_train_resampled, y_train_pred)
test_conf_matrix = confusion_matrix(y_test, y_test_pred)
print("Training Confusion Matrix:\n", train_conf_matrix)
print("Testing Confusion Matrix:\n", test_conf_matrix)

train_class_report = classification_report(y_train_resampled, y_train_pred)
test_class_report = classification_report(y_test, y_test_pred)
print("Training Classification Report:\n", train_class_report)
print("Testing Classification Report:\n", test_class_report)

train_kappa = cohen_kappa_score(y_train_resampled, y_train_pred)
test_kappa = cohen_kappa_score(y_test, y_test_pred)
print("Training Cohen's Kappa:", train_kappa)
print("Testing Cohen's Kappa:", test_kappa)

train_roc_auc = roc_auc_score(y_train_resampled, svm.predict_proba(x_train_resampled)[:, 1])
test_roc_auc = roc_auc_score(y_test, svm.predict_proba(x_test)[:, 1])
print("Training ROC-AUC Score:", train_roc_auc)
print("Testing ROC-AUC Score:", test_roc_auc)

def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(train_conf_matrix, "Training Confusion Matrix")
plot_confusion_matrix(test_conf_matrix, "Testing Confusion Matrix")

feature_importances = np.abs(svm.coef_[0])
features = x_train.columns
importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='pastel')
plt.title('Feature Importance (SVM Coefficients)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()